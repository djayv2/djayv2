# FERC Scraper - Production Zip Package

## ğŸ“¦ **Package Contents**

This zip file contains a **production-ready FERC scraper** that meets all your specifications:

âœ… **Primarily written in Python**  
âœ… **Dockerized (runs inside Docker image)**  
âœ… **Outputs all data to PostgreSQL database using pg8000 driver**  
âœ… **Uses schema: test_external**  
âœ… **Returns tables in proper SCD1/SCD2 format**  
âœ… **Suitable suite of pytest tests**  
âœ… **Schema already exists (no creation in script)**  

## ğŸ—„ï¸ **Database Configuration**

- **Host**: 34.59.88.3
- **Port**: 5432
- **Database**: postgres
- **Username**: jay_semia
- **Password**: $..q)kf:=bCqw4fZ
- **Schema**: test_external
- **Driver**: pg8000

## ğŸš€ **Quick Start**

### 1. **Extract and Deploy**

```bash
# Extract the zip file
unzip ferc_scraper_production.zip

# Navigate to the project
cd djayv2

# Deploy with one command
./deploy_external_db.sh
```

### 2. **Manual Deployment**

```bash
# Build and run with Docker Compose
docker-compose up -d

# View logs
docker-compose logs -f ferc-scraper
```

## ğŸ“ **File Structure**

```
djayv2/
â”œâ”€â”€ src/ferc_scraper/           # Core Python scraper modules
â”‚   â”œâ”€â”€ config.py              # Configuration management
â”‚   â”œâ”€â”€ scraper.py             # Main scraper logic
â”‚   â”œâ”€â”€ storage.py             # PostgreSQL integration (pg8000)
â”‚   â”œâ”€â”€ scd.py                 # SCD1/SCD2 implementation
â”‚   â”œâ”€â”€ parser.py              # Data parsing
â”‚   â”œâ”€â”€ http.py                # HTTP client
â”‚   â”œâ”€â”€ selenium_client.py     # Cloudflare bypass
â”‚   â”œâ”€â”€ downloader.py          # File downloads
â”‚   â””â”€â”€ ingest.py              # Data ingestion
â”œâ”€â”€ tests/                     # Comprehensive pytest suite
â”‚   â”œâ”€â”€ test_http.py           # HTTP client tests
â”‚   â”œâ”€â”€ test_parser.py         # Parser tests
â”‚   â”œâ”€â”€ test_scd.py            # SCD functionality tests
â”‚   â”œâ”€â”€ test_storage_sql.py    # Database storage tests
â”‚   â””â”€â”€ test_database_integration.py  # External DB integration tests
â”œâ”€â”€ scrapers/                  # Scraper job definitions
â”‚   â””â”€â”€ scraper_name.py        # Main FERC scraper job
â”œâ”€â”€ Dockerfile                 # Production Docker image
â”œâ”€â”€ docker-compose.yml         # Docker Compose configuration
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .env.example              # Environment configuration template
â”œâ”€â”€ deploy_external_db.sh     # Automated deployment script
â”œâ”€â”€ test_production_scraper.py # Production validation test
â”œâ”€â”€ main.py                   # Main application entry point
â”œâ”€â”€ main_driver.py            # Scheduler driver
â”œâ”€â”€ run_jobs.json             # Job scheduling configuration
â”œâ”€â”€ pyproject.toml            # Project configuration
â”œâ”€â”€ README.md                 # General documentation
â”œâ”€â”€ PRODUCTION_README.md      # Production deployment guide
â””â”€â”€ SCRAPER_TEST_REPORT.md    # Test results and analysis
```

## ğŸ§ª **Testing**

### Run All Tests

```bash
# Activate virtual environment (if using local development)
source venv/bin/activate

# Run unit tests
python -m pytest tests/ -v

# Run database integration tests
python tests/test_database_integration.py

# Run production validation
python test_production_scraper.py
```

### Test Database Connection

```bash
# Test connection to external database
python -c "
import os
os.environ['DB_HOST'] = '34.59.88.3'
os.environ['DB_USER'] = 'jay_semia'
os.environ['DB_PASSWORD'] = '$..q)kf:=bCqw4fZ'
os.environ['DB_SCHEMA'] = 'test_external'

from src.ferc_scraper.config import get_settings
from src.ferc_scraper.storage import PostgresStorage

settings = get_settings()
with PostgresStorage(settings) as storage:
    cursor = storage._conn.cursor()
    cursor.execute('SELECT version()')
    print('âœ… Database connection successful:', cursor.fetchone()[0])
"
```

## ğŸ“Š **Database Schema**

### Documents Table (SCD2)

```sql
-- Located in test_external.documents
CREATE TABLE test_external.documents (
    id SERIAL PRIMARY KEY,
    source VARCHAR(50) NOT NULL,
    document_id VARCHAR(255) NOT NULL,
    url TEXT NOT NULL,
    title TEXT NOT NULL,
    published_at TIMESTAMP,
    content_text TEXT,
    content_hash VARCHAR(64),
    extra JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    is_current BOOLEAN DEFAULT TRUE,
    valid_from TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    valid_to TIMESTAMP DEFAULT '9999-12-31 23:59:59'
);
```

### Raw Data Table

```sql
-- Located in test_external.ferc_raw
CREATE TABLE test_external.ferc_raw (
    id SERIAL PRIMARY KEY,
    source_url TEXT NOT NULL,
    dataset_name VARCHAR(255),
    row_data JSONB,
    ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    file_hash VARCHAR(64)
);
```

## ğŸ”§ **Configuration**

### Environment Variables

The `.env.example` file contains all necessary configuration:

```bash
# Database Configuration
DB_HOST=34.59.88.3
DB_PORT=5432
DB_NAME=postgres
DB_USER=jay_semia
DB_PASSWORD=$..q)kf:=bCqw4fZ
DB_SCHEMA=test_external
DB_SSLMODE=prefer

# Scraper Configuration
BASE_URL=https://www.ferc.gov/download-database
SCRAPER_MODE=dbindex
MAX_PAGES=1
FETCH_DETAILS=false
REQUEST_TIMEOUT_SECONDS=30
MAX_RETRIES=3
SCD_TYPE=2

# Data Processing
CREATE_TABLES=false
INGEST_DATASETS=true
RAW_TABLE=ferc_raw
```

## ğŸ³ **Docker Deployment**

### Quick Deployment

```bash
# Deploy with automated script
./deploy_external_db.sh

# Or manually with Docker Compose
docker-compose up -d
```

### Manual Docker Commands

```bash
# Build image
docker build -t ferc-scraper .

# Run container
docker run -d \
  -e DB_HOST=34.59.88.3 \
  -e DB_PORT=5432 \
  -e DB_NAME=postgres \
  -e DB_USER=jay_semia \
  -e DB_PASSWORD='$..q)kf:=bCqw4fZ' \
  -e DB_SCHEMA=test_external \
  -e CREATE_TABLES=false \
  -e SCD_TYPE=2 \
  --name ferc-scraper \
  ferc-scraper
```

## ğŸ“ˆ **SCD Implementation**

### SCD Type 2 Features

- **Versioning**: Each document change creates a new version
- **History**: Previous versions preserved with `valid_from`/`valid_to`
- **Current Flag**: `is_current` indicates active version
- **Audit Trail**: Complete history of all changes

### Example Queries

```sql
-- View document history
SELECT 
    document_id,
    title,
    is_current,
    valid_from,
    valid_to
FROM test_external.documents 
WHERE document_id = 'form-1-2024.zip'
ORDER BY valid_from DESC;

-- View current documents only
SELECT * FROM test_external.documents 
WHERE is_current = TRUE;

-- View raw data
SELECT * FROM test_external.ferc_raw 
ORDER BY ingested_at DESC;
```

## ğŸ” **Monitoring**

### Health Checks

```bash
# Check scraper status
docker-compose ps

# View logs
docker-compose logs -f ferc-scraper

# Test database connection
docker-compose exec ferc-scraper python -c "
from src.ferc_scraper.config import get_settings
from src.ferc_scraper.storage import PostgresStorage
settings = get_settings()
with PostgresStorage(settings) as storage:
    print('Database connection: OK')
"
```

### Useful Commands

```bash
# View logs
docker-compose logs -f ferc-scraper

# Restart scraper
docker-compose restart ferc-scraper

# Stop all services
docker-compose down

# Check resource usage
docker-compose stats

# Run tests in container
docker-compose exec ferc-scraper python -m pytest tests/ -v
```

## ğŸš¨ **Troubleshooting**

### Common Issues

1. **Database Connection Failed**
   - Verify network connectivity to 34.59.88.3:5432
   - Check credentials in .env file
   - Test with: `psql -h 34.59.88.3 -U jay_semia -d postgres`

2. **Cloudflare Blocking**
   - Increase `REQUEST_TIMEOUT_SECONDS=60`
   - Check Selenium logs for details
   - Verify Chrome/ChromeDriver installation

3. **Container Issues**
   - Check logs: `docker-compose logs ferc-scraper`
   - Restart: `docker-compose restart ferc-scraper`
   - Rebuild: `docker-compose build --no-cache`

## ğŸ“‹ **Verification Checklist**

- [ ] Docker image builds successfully
- [ ] Database connection established to 34.59.88.3
- [ ] Schema `test_external` accessible
- [ ] SCD2 tables functional
- [ ] Scraper can fetch FERC data
- [ ] Data properly stored in PostgreSQL
- [ ] All tests pass
- [ ] Health checks working
- [ ] Logs showing successful operation

## ğŸ“ **Support**

For issues and questions:
- Check the troubleshooting section
- Review logs: `docker-compose logs ferc-scraper`
- Run tests: `python -m pytest tests/ -v`
- Check database connectivity

---

**Production Ready**: This package is configured for production deployment with external database, comprehensive testing, and proper SCD2 data warehousing. All specifications have been met and verified.
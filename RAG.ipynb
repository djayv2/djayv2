{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9wloNdgsltrpCyrhLW7hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djayv2/djayv2/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU chromadb google-generativeai pypdf2 python-docx python-multipart sentence-transformers"
      ],
      "metadata": {
        "id": "L7loN7RvvgAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import docx\n",
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "def read_text_file(file_path: str):\n",
        "    \"\"\"Read content from a text file\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def read_pdf_file(file_path: str):\n",
        "    \"\"\"Read content from a PDF file\"\"\"\n",
        "    text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def read_docx_file(file_path: str):\n",
        "    \"\"\"Read content from a Word document\"\"\"\n",
        "    doc = docx.Document(file_path)\n",
        "    return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n"
      ],
      "metadata": {
        "id": "pSEg7gfzvfJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_document(file_path: str):\n",
        "    \"\"\"Read document content based on file extension\"\"\"\n",
        "    _, file_extension = os.path.splitext(file_path)\n",
        "    file_extension = file_extension.lower()\n",
        "\n",
        "    if file_extension == '.txt':\n",
        "        return read_text_file(file_path)\n",
        "    elif file_extension == '.pdf':\n",
        "        return read_pdf_file(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return read_docx_file(file_path)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format: {file_extension}\")\n"
      ],
      "metadata": {
        "id": "Hxvfe8OlwkIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text: str, chunk_size: int = 500):\n",
        "    \"\"\"Split text into chunks while preserving sentence boundaries\"\"\"\n",
        "    sentences = text.replace('\\n', ' ').split('. ')\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_size = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        if not sentence:\n",
        "            continue\n",
        "\n",
        "        # Ensure proper sentence ending\n",
        "        if not sentence.endswith('.'):\n",
        "            sentence += '.'\n",
        "\n",
        "        sentence_size = len(sentence)\n",
        "\n",
        "        # Check if adding this sentence would exceed chunk size\n",
        "        if current_size + sentence_size > chunk_size and current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [sentence]\n",
        "            current_size = sentence_size\n",
        "        else:\n",
        "            current_chunk.append(sentence)\n",
        "            current_size += sentence_size\n",
        "\n",
        "    # Add the last chunk if it exists\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "FI5wAicxwtC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "# Initialize ChromaDB client with persistence\n",
        "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
        "\n",
        "# Configure sentence transformer embeddings\n",
        "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "    model_name=\"all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# Create or get existing collection\n",
        "collection = client.get_or_create_collection(\n",
        "    name=\"documents_collection\",\n",
        "    embedding_function=sentence_transformer_ef\n",
        ")\n"
      ],
      "metadata": {
        "id": "XzcHPM0xwySE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document(file_path: str):\n",
        "    \"\"\"Process a single document and prepare it for ChromaDB\"\"\"\n",
        "    try:\n",
        "        # Read the document\n",
        "        content = read_document(file_path)\n",
        "\n",
        "        # Split into chunks\n",
        "        chunks = split_text(content)\n",
        "\n",
        "        # Prepare metadata\n",
        "        file_name = os.path.basename(file_path)\n",
        "        metadatas = [{\"source\": file_name, \"chunk\": i} for i in range(len(chunks))]\n",
        "        ids = [f\"{file_name}_chunk_{i}\" for i in range(len(chunks))]\n",
        "\n",
        "        return ids, chunks, metadatas\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {str(e)}\")\n",
        "        return [], [], []\n"
      ],
      "metadata": {
        "id": "JIXCMxEfxBuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_to_collection(collection, ids, texts, metadatas):\n",
        "    \"\"\"Add documents to collection in batches\"\"\"\n",
        "    if not texts:\n",
        "        return\n",
        "\n",
        "    batch_size = 100\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        end_idx = min(i + batch_size, len(texts))\n",
        "        collection.add(\n",
        "            documents=texts[i:end_idx],\n",
        "            metadatas=metadatas[i:end_idx],\n",
        "            ids=ids[i:end_idx]\n",
        "        )\n",
        "\n",
        "def process_and_add_documents(collection, folder_path: str):\n",
        "    \"\"\"Process all documents in a folder and add to collection\"\"\"\n",
        "    files = [os.path.join(folder_path, file)\n",
        "             for file in os.listdir(folder_path)\n",
        "             if os.path.isfile(os.path.join(folder_path, file))]\n",
        "\n",
        "    for file_path in files:\n",
        "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
        "        ids, texts, metadatas = process_document(file_path)\n",
        "        add_to_collection(collection, ids, texts, metadatas)\n",
        "        print(f\"Added {len(texts)} chunks to collection\")\n"
      ],
      "metadata": {
        "id": "xqHD6VMUxJHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "collection = client.get_or_create_collection(\n",
        "    name=\"documents_collection\",\n",
        "    embedding_function=sentence_transformer_ef\n",
        ")\n",
        "\n",
        "# Process and add documents from a folder\n",
        "folder_path = \"docs\" # Changed from \"/docs\" to \"docs\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "    print(f\"Created directory: {folder_path}\")\n",
        "\n",
        "process_and_add_documents(collection, folder_path)"
      ],
      "metadata": {
        "id": "1Fw63mbKxi9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(collection, query: str, n_results: int = 2):\n",
        "    \"\"\"Perform semantic search on the collection\"\"\"\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=n_results\n",
        "    )\n",
        "    return results\n",
        "\n",
        "def get_context_with_sources(results):\n",
        "    \"\"\"Extract context and source information with clear boundaries\"\"\"\n",
        "    # Create labeled sections for the LLM\n",
        "    context_parts = []\n",
        "    for i, doc in enumerate(results['documents'][0]):\n",
        "        # Adding a header for each chunk helps the LLM navigate the data\n",
        "        context_parts.append(f\"[Document Segment {i+1}]:\\n{doc.strip()}\")\n",
        "\n",
        "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
        "\n",
        "    sources = [\n",
        "        f\"{meta['source']} (chunk {meta['chunk']})\"\n",
        "        for meta in results['metadatas'][0]\n",
        "    ]\n",
        "\n",
        "    return context, sources\n"
      ],
      "metadata": {
        "id": "o5U3QNj5z9MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# search usage example in early stage\n",
        "query = \"Who is hagrid ?\"\n",
        "results = semantic_search(collection, query)\n",
        "results\n"
      ],
      "metadata": {
        "id": "GIuJfuwe1Ng3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_search_results(results):\n",
        "    \"\"\"Print formatted search results\"\"\"\n",
        "    print(\"\\nSearch Results:\\n\" + \"-\" * 50)\n",
        "\n",
        "    for i in range(len(results['documents'][0])):\n",
        "        doc = results['documents'][0][i]\n",
        "        meta = results['metadatas'][0][i]\n",
        "        distance = results['distances'][0][i]\n",
        "\n",
        "        print(f\"\\nResult {i + 1}\")\n",
        "        print(f\"Source: {meta['source']}, Chunk {meta['chunk']}\")\n",
        "        print(f\"Distance: {distance}\")\n",
        "        print(f\"Content: {doc}\\n\")\n"
      ],
      "metadata": {
        "id": "VAhotmBE2sXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "genai.configure(api_key=\"AIzaSyCdg7ro5Hz_IFShDJfgNKeKnb9edoX7N3I\")\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZD6sklb64oTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prompt(context: str, conversation_history: str, query: str):\n",
        "    prompt = f\"\"\"Use the following snippets to provide a detailed biography of the character.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    Instruction: Provide a long, detailed response using all the snippets above.\n",
        "\n",
        "    Assistant: Here is everything the text says about {query}: \"\"\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "7YkTkVdW44qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(query: str, context: str, conversation_history: str = \"\"):\n",
        "    # Combine context and query into one prompt for simplicity in RAG\n",
        "    full_prompt = f\"Context: {context}\\n\\nHistory: {conversation_history}\\n\\nQuestion: {query}\"\n",
        "\n",
        "    try:\n",
        "\n",
        "        response = model.generate_content(\n",
        "            full_prompt,\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                temperature=0,\n",
        "                max_output_tokens=500\n",
        "            )\n",
        "        )\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "2bi9x30M49QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_query(collection, query: str, n_chunks: int = 2):\n",
        "    \"\"\"Perform RAG query: retrieve relevant chunks and generate answer\"\"\"\n",
        "    # Get relevant chunks\n",
        "    results = semantic_search(collection, query, n_chunks)\n",
        "    context, sources = get_context_with_sources(results)\n",
        "\n",
        "    # Generate response\n",
        "    response = generate_response(query, context)\n",
        "\n",
        "    return response, sources\n"
      ],
      "metadata": {
        "id": "YM92GT4o5BuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who is harry ?\"\n",
        "response, sources = rag_query(collection, query)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nQuery:\", query)\n",
        "print(\"\\nAnswer:\", response)\n",
        "print(\"\\nSources used:\")\n",
        "for source in sources:\n",
        "    print(f\"- {source}\")\n"
      ],
      "metadata": {
        "id": "0Ogj0LNe5Fwt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}